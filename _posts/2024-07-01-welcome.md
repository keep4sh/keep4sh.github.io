---
title: 첫 경진대회
date: 2024-06-07 11:32:42 +09:00
categories: [AI, Machine Learning]
tags: [AI, ML, Machine Learning, Machinelearning, MachineLearnging, Study]		# TAG는 반드시 소문자로 이루어져야함!
---

# 1. 머릿말

markdown에 익숙해질 겸, 첫 경진대회 마무리 겸 처음으로 포스팅을 해보겠습니다.

처음으로 현재 진행되는 대회를 참가해봤습니다.
참가 했던 대회의 데이터 Shape은 다음과 같습니다.

    Train : 100,000
    Test : 35,816
    columns : 76

Column들이 많아 보이긴 하지만 사전에 Encoding 된 데이터를 주셔서 크게 많지는 않았습니다. (30개 정도?)    


# 2. 진행 과정
   
2주 정도의 시간을 주고 데이터를 보면서 모델을 선정하고 예측하는 과정을 진행했습니다.
데이터 안에서 column의 설명만 봐도 서로 연관성이 높아 보이는 데이터도 많았고
이상치, 결측치가 없는 깨끗한 데이터여서 전처리 과정을 타이트하게 가져가지는 않았습니다.

진행 과정 중 사용한 전처리와 라이브러리는 다음과 같습니다.

~~~python

# Importing scikit learn library

## Encoder / Scaler
from sklearn.preprocessing import RobustScaler
from category_encoders import BinaryEncoder

# Model Selection
from sklearn.model_selection import KFold, StratifiedKFold, GroupKFold, StratifiedGroupKFold, train_test_split

# Dimension Reduction
from sklearn.decomposition import PCA

# VIF 측정
from statsmodels.stats.outliers_influence import variance_inflation_factor

# F_1 Score
from sklearn.metrics import f1_score
~~~

## 1-1. Encoding
우선 Feature들 중 이미 Encoding 된 데이터 외에 Encoding이 필요해 보이는 것들도 있었습니다.   
어떠한 Encoding을 해야할까? 라는 고민은 다음과 같이 진행 했습니다.   

데이터 상 Class 별로 찢을 시 적게는 5개에서 많게는 70개? 정도로 찢어질 것 같았습니다.   
때문에 너무 많은 Feature가 생기는 것을 방지하고자 Binary Encoding을 사용했습니다.   

Ohe의 경우 변수의 Class 별로 Column을 만들기에 너무 많은 변수가 생기지만
Binary Encoder의 경우 변수의 종류를 이진 분류를 통해서 정리하니 보다 적은 수의 Column이 생성됩니다.

## 1-2. Feature Engineering
"Feature 들 중 몇몇 특성을 조합해서 각 개인의 변제 의사 / 변제 능력을 수치화 할 수 있지 않을까?"
위 생각을 기본으로 여러 Feature들을 조합해보고자 Gpt, 주변에 도움을 얻어 수치화를 성공했습니다.

~~~python
# 변제 의사
ori_train['dti_s'] = ori_train['dti'].replace(0, 1e-6)
ori_train['repayment_capacity_score'] = ((ori_train['annual_inc'] / ori_train['dti_s']) + ori_train['fico_range_high'] - ori_train['revol_bal'] + ori_train['tot_cur_bal'] + ori_train['total_acc'] * 10).round(2)

# 변제 능력 추가
ori_train['repayment_willingness_score'] = 100 - (ori_train['delinq_2yrs'] * 5 + ori_train['acc_now_delinq'] * 20 + (12 - ori_train['mths_since_last_delinq']) * 2 + ori_train['pub_rec'] * 30 + ori_train['collections_12_mths_ex_med'] * 10 + ori_train['tot_coll_amt'] / 1000 + ori_train['inq_last_6mths'] * 3)
~~~

> __변제 의사__   
> 연소득 / 소득 대비 부채의 비율 + Fico range가 높았을 때 기준 - 리볼빙 잔액 + 전 계좌의 현재 통합 잔고 + 소유했던 신용카드의 갯수   
   

> __변제 능력__    
> 100 - (지난 2년간 체납 발생 횟수) * 5 + (체납되지 않은 계좌의 수) * 20 + (12-마지막 체납 일자) * 2 + 파산 횟수 *30 + 추심 발생 횟수 *10 + (총 추심액 /1000 + 신용 조회수 *3)

아무래도 미국 금융 관련 지식이 부족했어서 해당 수식이 정확하다고 판단하기는 어려우나 우선 모델에 어떻게 작용할 지 알 수 있지 않을까? 생각하며 추가 하였습니다.
다만 변제 능력의 경우 Survey의 형태로 이미 나눠져 있는 데이터도 있어서 이를 하나로 합쳐줬어야 하나 시간이 부족해 따로 진행하지 못했습니다.


## 1-3. VIF 측정
Numerical Column들에 대해서만 따로 VIF 측정을 진행, VIF 수치가 높았던 9개의 칼럼은 삭제를 진행 했습니다.   
Feature의 설명 중 유사하게 보이는 Column들이 2개 에서 3개 정도는 데이터 자체 확인 시에도 비슷한 분포, 데이터를 갖고 있었기에 삭제하였으며 VIF 측정하는 함수를 사용하여 측정하였습니다.   
이렇게 칼럼을 지우고 들었던 생각이 여기서 VIF 측정 후 바로 지우지 말고 각각 지워보면서 실험을 진행하였다면 어땠을까? 하는 아쉬움이 남습니다.   
아래는 사용했던 함수입니다.   
~~~python
def check_vif(df):
    vifs = [variance_inflation_factor(df, i) for i in range(df.shape[1])]
    vif_df = pd.DataFrame({"features":df.columns, "VIF" : vifs})
    vif_df = vif_df.sort_values(by="VIF", ascending=False)
    remove_col = vif_df.iloc[0, 0]
    top_vif = vif_df.iloc[0, 1]
    return vif_df, remove_col, top_vif
~~~

## 1-4. Scaling
VIF로 제외한 칼럼들 외에 공통적으로 Robust Scaling을 진행했습니다.   
데이터 마다 __이상치__ 들이 존재했고 이를 최대한 덜 반영하기 위해 __Median__ 값으로 Scaling을 진행하는 __Robust Scaling__ 을 진행했습니다.


## 2-1. Model Selection
Classification Task에서 제일 많이 사용하는 __XGBoost__ 만 사용했습니다.
XGB, CatBoost, Lgbm 등 다양한 모델을 사용하면서 실험을 해보았으면 더 좋은 경험이 되었을 거 같습니다.  

## 2-2. Cross Validation
Label Data가 약 60,000개 / 약 35,000개 정도로 차이가 났습니다.   
이 과정에서 __Label Data의 불균형__ 을 판단하셨던 분들도 많이 계셨습니다.   
다만 제 생각에 큰 차이가 나는 지표라는 생각은 하지 못해 따로 __OverSampling__ 혹은 __Cost-Sensitive Learning__ 을 진행하지 않았습니다.
       
__K-Fold__ 방식을 통해서 통해서 검증을 시도했습니다.   
이 과정 중 
> __K-Fold__ 로 학습이 잘되었던 모델에 대해서 이를 해당 모델이 학습했던 데이터만을 가지고 학습을 진행해도 괜찮은가?    
> 또 진행한다면 해당 모델이 학습한 데이터를 어떻게 가져오는가?"

위 와 같은 의문이 들어서 CV 방식을 변경하기로 결정 하였습니다.  

Label Data가 균일하게 나눠지지 않아 0만 학습한 Fold들이 생겼기 때문입니다.

이를 해결하고자 __Starified K-Fold__ 방식을 선택해서 진행했습니다.   
기본적인 진행은 K-Fold와 동일하나 __Label Data의 비율에 맞게__ train_test를 찢고 시험합니다.   

여기서 학습했던 모델 중 가장 잘 학습된 모델을 가져왔어야 하는데 이 과정은 제대로 하지 못하고 다음으로 넘어갔습니다.   
보완 후 해당 내용 재작성 하겠습니다.   

~~~python
skf = StratifiedKFold(n_splits=5)
model = XGBClassifier()

for i, (train_index, test_index) in enumerate(skf.split(X, y)):
    print(f"Fold {i}:")
    print(f"  Train: index={train_index}")
    print(f"  Test:  index={test_index}")

    X_train = X.iloc[train_index, :]
    y_train = y.iloc[train_index]
        
    X_val = X.iloc[test_index, :]
    y_val = y.iloc[test_index]
    
    model.fit(X_train, y_train)
    preds = model.predict(X_val)
    preds_proba = model.predict_proba(X_val)
    
    evaluation_metric = f1_score(y_val, preds, average='macro')
    print(f'Macro F1 Score : {evaluation_metric}')
~~~

## 2-2. HyperParameter Tunning
기존에는 사용해보지 않았던 __Optuna__ 를 사용해서 __Hyper Parameter__ 를 Tunning 하였습니다.   
딱히 범위를 조정하거나 바꾸지는 않았고 아래 코드 블럭 내용을 그대로 사용 했습니다.

```python
def xgb_optimizer(trial, X, y, K):
    
    n_estimators = trial.suggest_int('n_estimators', 100, 1000)
    max_depth = trial.suggest_int('max_depth', 4, 10)
    colsample_bytree = trial.suggest_categorical('colsample_bytree', [0.5, 0.6, 0.7, 0.8])
    learning_rate = trial.suggest_float('learning_rate', 1e-3, 1e-2)
    reg_lambda = trial.suggest_float('reg_lambda', 0.1, 2.0)
    
    
    model = XGBClassifier(n_estimators=n_estimators,
                          max_depth=max_depth,
                          colsample_bytree=colsample_bytree,
                          learning_rate=learning_rate,
                          reg_lambda=reg_lambda)
#                          scale_pos_weight=4.71)  ## we set class imbalance by using sampling method.
    
    
    folds = StratifiedKFold(n_splits=K, shuffle=True)
    losses = []
    
    for train_idx, val_idx in folds.split(X, y):
        X_train = X.iloc[train_idx, :]
        y_train = y.iloc[train_idx]
        
        X_val = X.iloc[val_idx, :]
        y_val = y.iloc[val_idx]
        
        model.fit(X_train, y_train)
        preds = model.predict(X_val)
        loss = evaluation_metric(y_val, preds, average='macro')
        losses.append(loss)
    
    
    return np.mean(losses)
```

이 부분에서도 아직 제가 갖고 있는 지식은 없으나 좋은 것이라고 생각해서 그냥 사용해본 느낌이 컷습니다.
Optuna 관련에 대해서도 작게나마 정리 해보겠습니다.

---
# 마무리
위 과정들을 전부 마무리하고 최적의 모델을 찾아서 제출한 것이 이번 경진대회의 끝입니다.
전체적으로 1주라는 타이트한 시간을 활용해서 얼마나 모델을, 데이터를 많이 다뤄보는지가 입상에 차이를 가르지 않았나 싶습니다.

## 개인적인 Review
사실 Review의 모습 보다는 아쉬움이 더 맞는 것 같습니다.   
데이터에 대한 분석을 깊이 있게 하지 못하고 날림 식으로 작성한 것들이 많은 것 같습니다.   
특히 Data의 __Encoding__ 과 __Scaling__ 에서 각 "" __데이터__ "" 의 특징에 맞게 진행 해볼걸...   
또 데이터를 CSV로 미리 만들어두거나 버전을 관리하는 등 실험을 위한 작업들이 적었던 것 같습니다.   
원 데이터에도 제대로 집중을 하거나 에너지를 쏟지 못하는데 새로운 Column들을 만든들 제대로 활용하지 못하는 느낌입니다.  

__둘 째__ 제 자신도 잘 이해하고 있지 못한 Optuna를 사용함에 있어 아쉬움 입니다.
Optuna 자체를 사용하면서 찾아봤던 많은 내용들 중 초기 ML Setting을 무척 상세하게 해두었던 Notebook을 찾았습니다.   
해당 Notebook을 무작정 따라하다보니 Setting의 차이로 제대로 동작하지 않는 일들이 많이 생겼습니다.   
Error들은 하나하나 수정해서 작동은 하였으나 제가 환경을 만들어서 세팅해야겠다는 다짐이 들었습니다.   

마지막으로   
__비록 이번에는 순위권에서 완전히 밖이지만 다음 경진대회에서는 10등 내외의 성적을 거두고 싶다는 생각이 듭니다.__   