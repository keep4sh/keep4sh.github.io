---
title: Normalization
date: 2024-06-18 13:00:00 +09:00
categories: [AI, Deep Learning]
tags: [AI, ML, Deep Learning, Deeplearning, DeepLearnging, Study, Yeardream, Noramalization]		# TAG는 반드시 소문자로 이루어져야함!
use_math: true
---
<p align = center>주요 개념</p>
> L1, L2 Noramalization
> L_p Norm

# 0. Normalization이 필요한 이유.
정규화 과정이 필요한 이유는 무엇일까? 우리에게 17세 남성 청소년들의 키와 몸무게의 상관관계를 정규분포 형태로 보여주는 데이터가 있다고 생각해보자. 이 데이터를 모델에 학습 시켜서 11세 어린이들 혹은 동일한 17세 여성 청소년들의 키, 몸무게는 모델의 학습된 데이터와 다른 분포를 가질 것이며 우리의 모델은 두 데이터에 대한 예측이 원활히 수행되지 않을 것이다. 즉, 우리의 모델의 일반화를 위해서 정규화 과정이 필요하다.   
정규화 과정이 필요한 이유는 더 있다. 우리가 모델에 이미지 데이터를 학습시킨다고 생각해보자. 우리의 이미지 데이터가 32x32 픽셀의 사이즈이고 RGB 3개의 채널을 통해서 표현된다고 정의하자. 예를 들어 (32, 200, 16)과 같은 형태의 픽셀이 존재하는 것이다.   
이러한 형태의 데이터를 모델에 학습시킬 때 몇가지 문제가 발생한다.    
1. 픽셀 데이터 안의 수치가 크기 때문에 Gradient의 크기도 크게 잡힐 가능성이 있다. 이는 Gradient Descent가 불안정하게 진행될 가능성이 있게 된다.
2. 데이터 내에 몇몇 큰 수치들로 인해서 학습이 편향될 가능성이 있다. 모델은 수치가 무엇을 의미하는지 정확히 인지하지 못한다. 다만 수치를 통해서 대소 비교가 가능하며 이를 통해 더 큰 수치는 연산 시 더 크게 Output이 형성된다. 이로 인해 우리의 모델은 데이터 전체를 균등하게 학습하지 못할 가능성이 있다.

# 1. $L_p$ Norm
위 설명을 통해서 Normalization이 필요한 이유는 어느 정도 와닿았을 것이라 생각한다. 그렇다면 기본적인 $L_p \text{Norm}$에 대해서 설명해보자.

$$
\mathbf{x} = (x_1, x_2, \ldots, x_n) \\ 
||\mathbf{x}||p = (\sum i = 1^n|xi|)^{1/p}
$$

위 수식은 $L_p$ Norm의 수식적 정의이다.    
수식에서 x들은 모두 vector의 구성요소이다. 위 수식을 통해서 우리는 서로 다른 벡터의 대소를 비교할 수 있게 된다.
$p$ 에 따라서 L1, L2, L $\infin$ 으로 불리며 각 Norm에 따라서 Vector에 대소를 비교할 때 특징을 가지게 된다.


# 2. L1 Norm

L1 Norm은 아무런 거듭제곱 없이 두 vector의 요소들의 거리를 표현하는 절댓값의 합이다.
$$
||\mathbf{x}||1 = \sum i = 1^n|x_i|
$$

이 정규화는 맨헤튼 정규화라고도 불리면서 단순 벡터 상의 거리를 각 축에 따라서 계산한다. 때문에 이상치가 존재할 경우 해당 이상치에 **민감하게** 반응한다.


# 3. L2 Norm
L2 Norm은 vector의 각 요소에 대해서 피타고라스 정의와 같이 거리를 측정한다. 이는 두 벡터의 거리의 차이를 통해서 최단 거리를 구하는 모습처럼 보일 수 있다.

$$
||\mathbf{x}||2 = \sqrt{\sum i=1^n|x^2_i|}
$$

L2 Norm은 유클리드 거리라고 하며 각 벡터의 절대적 크기를 보이는 정규화이다. L2 Norm을 통해서 절대적 크기를 구하기에 이상치에 대해서 비교적 둔감한 특성을 지닌다.

# 4. L $\infty$ Norm
L $\infty$ Norm은 각 벡터의 최대값의 절댓값 표현한다.
$$||\mathbf{x}||\infty = \max{i} |x_i|$$
p가 커질수록 이상치에 둔감해지며 벡터의 최대값으로 벡터를 대표하게 된다.