---
title: DropOut
date: 2024-06-19 09:00:00 +09:00
categories: [AI, Deep Learning]
tags: [AI, ML, Deep Learning, Deeplearning, DeepLearnging, Study, Yeardream, DropOut]		# TAG는 반드시 소문자로 이루어져야함!
use_math: true
---

지난 포스팅에서 DeepLearning의 Batch Normalization을 공부했었다. 지금까지의 공부했던 내용을 정리하자면   
> Neural Network의 구조
> MLP, Fully Connected Layer
> Foward Propagation, Back Propagation
> Gradient Descent의 종류 (SGD, Mini-Batch Gradient Descent)
> Momentum, Ada Gradient, RMSProp, Adam
> Batch Normalization

이번 포스팅에서는 Drop Out을 포스팅 하겠다.

<p align=center>주요 개념</p>
> Dropout
> Ensemble
> Inverted dropout

# 1. Drop Out
 Forward Propagation에서 일정 확률로 선택된 **일부 Node 혹은 Neuron** 들의 결과값을 **0**으로 설정하는 것이다. 굳이 왜 결과값을 0으로 설정해야 하는 이유가 무엇일까?     
많은 정보를 갖고 모델이 학습을 할텐데 일부 정보들을 Masking 하여서 결과물을 예측한다. 여러 Case로 Data를 Masking하여서 Output이 여러 Case로 나올 것이다. 이를 Machine Learning에서 Ensemble처럼 여러 모델들의 결과를 합치는 모습과 유사하다.

실제 사용 예시를 보자.

```python
p = 0.9
def train_step(x): # ex ) 3 layer Foward Pass
    h1 = np.maximum(0, np.dot(W1,x)+b1)
    u1 = np.random.rand(*h1.shape) <p
    h1 *= u1
    h2 = np.maximum(o. np.dot(w2,h1) + b2)
    u2 = np.random.rand(*h2.shpae) < p
    h2 *= u2
    o = np.dot(w2, h2) + b3
    # Drop out은 Activation Function 이후에 연산 되어야 한다. 
```

위 예시에서 Layer를 통과한 이후에 p, 0.9 보다 작은 것들만 살려서 연산을 진행한다. p를 drop_out rate라고 언급하며 hyper parameter로 줄 수 있다. 위 과정은 Iteration 마다 다른 Node가 Drop 되어서 매번 다르게 데이터를 학습하는 결과가 되는 것이다. 


# 2. 추론 과정에서 Drop out
위 과정은 모델이 Data를 학습할 때의 방식이다. 추론 과정을 어떻게 진행할까?
기존에 3개의 Node를 통해 하나의 y를 연산할 때, Drop Out은 3개의 Node에서 진행된다. 이 Drop Out되는 경우의 수의 평균을 갖고 y에 대해서 연산하는 방식을 통해서 연산한다.

```python
def predict(x):
    h1 = np.maximum(0, np.dot(w1, x) + b1)
    h1 *= p
    h2 = np. maximum(0, np.dot(w2, h1) + b2)
    h2 *= p
    o = np.dot(w3, h2) + b3
```

생각 해보면 Drop Out을 진행하면서 학습된 Node에서는 Scale이 작아졌을 것이다. 이를 맞춰주기 위해서 평균값을 조정해주면서 Drop out Rate를 연산하는 방식이다.

# 3. Inverted Drop out
위에서 설명했던 방식은 Drop out Rate를 추론 과정에서 진행되는 방식이다. 이를 동일하게 학습 시에도 연산을 통해 Scale을 맞춰주는 방식도 존재한다. 1/p 를 연산하여서 drop out을 통해 작아진 Scale을 키워주는 것이다. 추론 과정에서 p를 같이 연산해주지 않아도 된다.

```python
p = 0.9
def train_step(x): # ex ) 3 layer Foward Pass
    h1 = np.maximum(0, np.dot(W1,x)+b1)
    u1 = np.random.rand(*h1.shape) <p
    h1 *= u1 / p
    h2 = np.maximum(o. np.dot(w2,h1) + b2)
    u2 = np.random.rand(*h2.shpae) < p
    h2 *= u2 / p
    o = np.dot(w2, h2) + b3
    # Drop out은 Activation Function 이후에 연산 되어야 한다. 
```